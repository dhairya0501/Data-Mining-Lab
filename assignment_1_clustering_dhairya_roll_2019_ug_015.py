# -*- coding: utf-8 -*-
"""Assignment_1_Clustering_Dhairya_ROLL-2019/UG/015.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vT0MzQ1s3vsgSPZ9iuWFHQqYhkzB4mwG

<p>
Download the Dataset from this <a href='https://drive.google.com/file/d/10I-2HLu_RZWN-ZnAUIpTDA0IsNm84gQk/view?usp=share_link'>link</a>
</p>

## About the DataSet

The `dataset.csv` dataset contains the following features:

Feature | Description 
----------|---------------
**`prject_id`** | A unique identifier for the proposed project. **Example:** `p036502`
**`teacher_id`** | A unique identifier for the teacher. **Example:** `c90749f5d961ff158d4b4d1e7dc665fc`   
**`project_topic`**    | Topic of the project. **Examples:**<br><ul><li><code>Art Will Make You Happy!</code></li><li><code>First Grade Fun</code></li></ul> 
**`grade_category`** | Grade level of students for which the project is targeted. One of the following enumerated values: <br/><ul><li><code>Grades PreK-2</code></li><li><code>Grades 3-5</code></li><li><code>Grades 6-8</code></li><li><code>Grades 9-12</code></li></ul>  
 **`project_subject_type`** | One or more subject categories for the project from the following enumerated list of values:  <br/><ul><li><code>Applied Learning</code></li><li><code>Care &amp; Hunger</code></li><li><code>Health &amp; Sports</code></li><li><code>History &amp; Civics</code></li><li><code>Literacy &amp; Language</code></li><li><code>Math &amp; Science</code></li><li><code>Music &amp; The Arts</code></li><li><code>Special Needs</code></li><li><code>Warmth</code></li></ul><br/> 
  **`school_state`** | State where school is located ([Two-letter U.S. postal code]). **Example:** `WY`
**`project_subject_subtype`** | One or more subject subcategories for the project. **Examples:** <br/><ul><li><code>Literacy</code></li><li><code>Literature &amp; Writing, Social Sciences</code></li></ul> 
**`project_proposal`**    | Proposal Text<sup>*</sup>   
**`teacher_salutation`** | Teacher's title. One of the following enumerated values: <br/><ul><li><code>nan</code></li><li><code>Dr.</code></li><li><code>Mr.</code></li><li><code>Mrs.</code></li><li><code>Ms.</code></li><li><code>Teacher.</code></li></ul>  
**`previously_submitted_proposals`** | Number of project applications previously submitted by the same teacher. **Example:** `2` 

Label | Description
----------|---------------
`project_is_accepted` | A binary value indicates whether the project proposal will be accepted or not. 0 means rejected and 1 means accepted

<ol> <strong>Instructions </strong>
<br>
You can use scikit-learn library to write python script for the following questions. You have to use google colaboratory and submit the colab notebook file as answer script. While writing the code you should give proper heading for each and every question. You have to install genism python library (version 4.2.0).
<br>
<strong>Questions:</strong>
    <li><strong>Choose the feature set(categorical, numerical features + project_topic [TF-IDF weighted word2vec] + project_proposal [TF-IDF weighted word2vec]).</li></strong>
    <li><strong>Word2Vec: Download the Glove pretrained word-embeddings from the <a href='https://nlp.stanford.edu/data/glove.6B.zip'>link</a>. Use 100d word vectors of glove embeddings while using the glove pretrained word-embedding properly handle the out of vocabulary(oov) word if it occurs.
    <li><strong>Use kmeans, Agglomerative clustering, DBSCAN</strong><br>
    - <strong>K-Means Clustering:</strong> <br>
        ● Find the best ‘k’ using the elbow-knee method (show the plot of # of clusters vs inertia_)<br>
    - <strong>Agglomerative Clustering: </strong><br>
        ● You have to apply agglomerative clustering algorithm and use number of clusters 3. Also visualize the dendogram plot.<br>
    - <strong>DBSCAN Clustering: </strong><br>
        ● Find the best ‘eps’ using the elbow-knee method. </li>
<li><strong> You have to plot the words present in the "proposal text" column for each clusters of the algorithms mentioned in point 3. </strong></li>

# **Set Path and Import Modules**
"""

import numpy as np
import pandas as pd
import gensim
import gensim.downloader
from gensim.test.utils import datapath, get_tmpfile
from gensim.models import KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec
glove_file = datapath('/content/glove.6B.100d.txt')
#word2vec_glove_file = get_tmpfile("/content/glove.6B.100d.txt")
#glove2word2vec(glove_file, word2vec_glove_file)

#practice
glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')

"""**Extract Data from CSV**"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/dataset.csv')

target=df['project_is_accepted']
df=df.drop(['project_is_accepted'],axis=1)
print(df.head())

text_data=df['project_topic'].fillna('')

from gensim.models import TfidfModel
from gensim.corpora import Dictionary
from gensim.utils import simple_preprocess

doc_tokenized = [simple_preprocess(doc) for doc in text_data]

dct = Dictionary() 
BoW = [dct.doc2bow(doc, allow_update=True) for doc in doc_tokenized]

tfidf = TfidfModel(BoW)

vocab=glove_vectors.vocab

l=[]
#freq_sum=[]
for i in range(len(text_data)):
  freq_sum=[]
  a=0
  for b in tfidf[BoW[i]]:
    #check whether the dct[b[0]] is in vocab
    if dct[b[0]] in vocab:
      wv = glove_vectors[dct[b[0]]]
      freq = b[1]
      a = a + np.dot(freq,wv)
      #freq_sum=freq_sum+freq
      freq_sum.append(freq)

  if len(freq_sum) !=0:

    tfidf_weighted = a/np.sum(freq_sum)
    l.append(tfidf_weighted)
  else:
    '''
    print(text_data[i], i)
    print('array sum ', np.sum(freq_sum))
    print("freq_sum ",freq_sum)
    '''
    l.append(np.zeros(100))

df['project_topic']=l

project_prop=df['project_proposal'].fillna('')
doc_tokenized = [simple_preprocess(doc) for doc in project_prop]

dct = Dictionary() 
BoW = [dct.doc2bow(doc, allow_update=True) for doc in doc_tokenized]

tfidf = TfidfModel(BoW)

proposal=[]
#print(proposal[0])
#freq_sum=[]
for i in range(len(project_prop)):
  freq_sum=[]
  a=0
  for b in tfidf[BoW[i]]:
    #check whether the dct[b[0]] is in vocab
    if dct[b[0]] in vocab:
      wv = glove_vectors[dct[b[0]]]
      freq = b[1]
      a = a + np.dot(freq,wv)
      #freq_sum=freq_sum+freq
      freq_sum.append(freq)

  if len(freq_sum) !=0:

    tfidf_weighted = a/np.sum(freq_sum)
    proposal.append(tfidf_weighted)
  else:
    proposal.append(np.zeros(100))

df['project_proposal']=proposal

"""**L2 Normalization for Numerical Data**"""

from sklearn.preprocessing import normalize
y=df[['previously_submitted_proposals']]
#print(y)
#y=y.values.reshape(-1,1)



y_l2 = normalize(y.values,axis=0)
df['previously_submitted_proposals']=y_l2
sum=0
for x in df['previously_submitted_proposals']:
  if x >=1:
    sum+=x

"""**One-Hot Representation for Categorical Data**"""

categorical= df.drop(['prject_id','teacher_id','project_topic','project_proposal','previously_submitted_proposals'],axis=1)
c=np.array(categorical.columns)

def encode_and_bind(original_dataframe, feature_to_encode):
    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])
    res = pd.concat([original_dataframe, dummies], axis=1)
    res = res.drop([feature_to_encode], axis=1)
    return(res)
for x in c:
  df = encode_and_bind(df,x)

"""**TFIDF-Weighted Word2Vec for Text Data**"""



"""**Data for Clustering**

**K-Means Clustering**
"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

#initialize kmeans parameters
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"random_state": 1,
}

#create list to hold SSE values for each k
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    array = df['project_proposal'].to_list()
    kmeans.fit(array)
    sse.append(kmeans.inertia_)

#visualize results
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()

"""***AS INFERRED FROM THE GRAPH WE WILL HAVE 4 CLUSTERS***

**Agglomerative Clustering**
"""

from sklearn.cluster import AgglomerativeClustering
clustering=AgglomerativeClustering(n_clusters=3,linkage='single',compute_distances=True)

clustering = clustering.fit(array)

from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)
plt.title("Hierarchical Clustering Dendrogram")
# plot the top three levels of the dendrogram
plot_dendrogram(clustering, truncate_mode="level", p=3)
plt.xlabel("Number of points in node (or index of point if no parenthesis).")
plt.show()

"""**DBSCAN Clustering**

**Reducing Dimensions so that DBSCAN can be done**
"""

from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
import scipy
pca = PCA(n_components=2)
p=scipy.sparse.csr_matrix(proposal)

clf = TruncatedSVD()
Xpca = clf.fit_transform(p)
#y = pca.transform(p)
y = pd.DataFrame(Xpca,columns=['PC1','PC2'])
y.head()

from sklearn.cluster import DBSCAN
dbscan =  DBSCAN(eps=1).fit(y)